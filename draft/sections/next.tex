\documentclass[../main.tex]{subfiles}

\begin{document}
\section{Discussion}

%"This chapter steps back from the details and considers the larger issues that have been raised. In particular, it considers how this research helps to accomplish the motivating goal of this dissertation, which is the development of computer-based graphical communication." - Mackinlay

\subsection{What have we done}
This work contributes a mathematical description of the transformation from data to visual representation. Combining Butler's fiber bundle model of data with Spivaks formalism of data schemas provides a way of decoupling topology from variability such that the model can support a very large variety of datasets, including discrete relational tables, multivariate high resolution spatio temporal datasets, and complex networks. Modeling the graphic as a fiber bundle provides a way to separate the target display space from the topology of the graphic. By decomposing the mapping from data to visual representation as encoding \vchannel, assembly \vmark, and a mapping between data and graphic topologies \vindex and formalizing what equivariance each stage needs to preserved, this work derives constraints that visualization library authors could embed in their code to guarantee visualizations that are equivariant transforms of the input data. 

This work generalizes previous research constraining visual encodings from data components to graphic components as equivariant maps to components that are N-dimensional. Furthermore, it precisely defines the glyph as the visual element constructed from data on a simplex in a simplacial complex where the simplex is discrete or continuous. This is a restatement of for example Bertin's definition of a line that encapsulates all points on the continuous line.  By modeling the data topology along with its variablity, this work also provides  a generalization of topology preservation as a deformation retraction from graphic space to data space. By using a functional paradigm, we can deconstruct the graphic to the glyph associated with each data point or even to pieces of a glyph; therefore the renderer has full  flexibility in how to to generate the image, while the graphic to data topology maps provide a way to keep track of which part of the image belongs to which data point. 

The toy prototype built using this model validates that is usable for a general purpose visualization tool since it can be iteratively integrated into the existing architecture rather than starting from scratch/ Factoring out glyph formation into assembly functions allows for much more clarity in how the glyphs differ. This prototype demonstrates that this framework can generate the fundemental marks, point (scatter plot), line (line chart), and area (bar chart). Furthermore, the grouped and stacked bar examples demonstrate that this model supports composition of glyphs into more complex graphics. These composite examples also rely on the fiber bundles section base book keeping to keep track of which components contribute to the attributes of the glyph. Implementing this example using a Pandas dataframe demonstrates the ease of incorporating existing widely used data containers rather than requiring users to conform to one stands.  

\subsection{What are the limitations?}
%%%An interesting control problem will be how to keep the text and graphics consistent. If the rendering exchanges the horizontal and vertical axes of a bar chart, the text must also exchange its references to the axes.
So far this model has only been worked out for a single data set tied to a primitive mark, but it should be extensible to compositing datasets and complex glyphs. The examples and prototype have so far only been implemented for the static 2D case, but nothing in the math limits to 2D and expansion to the animated case should be possible because the model is formalized in terms of the sheaf. Since it is a purely mathematical framework, this model does not have a way to model or check for semantic equivariance of figurative representations, but this could be an avenue of research for machine learning researchers. 

\begin{itemize}

\item can't handle figerative/semantic equivariance (this actually could maybe be done as extension of fber)
\item explicitely does not care about effectiveness - is domain specific 
\item is only developed against Matplotlib
\item is written against AGG/SVG
\item layout
\item text
\item same as APT, is at gd level so cannot control render 
\item keeping all data in sync if transform is changed in one place (so on multi composed graphs, changing x on does it propopgate - part of Bertin invariance)
\end{itemize}

\subsection{What else needs to be implemented?}
\begin{itemize}
    \item is compose(encodes) and ggplot is plot(encodes, glyph)
    \item heatmaps/ proof of concept of face simplex
    \item composition algebra
    \item boxplot - multiglyph composition
    \item debatable: non-trivial bundle like data on mobius backend
    \item interactivity 
    \item concurrent artist
    \item numpy wrapper
\end{itemize}
\begin{itemize}
    \item provisional implementation using new architecture in Matplotlib
    \item developer-aimed documentation (API + narrative)
    \item proof of concept 3rd party user-facing library with a biology partner
    \item academic paper about building visualization tools on top of the core architecture
\end{itemize}

Other things that may or may not play a big role would be working out the details of the S->K interaction for dashboards. By this I particularly mean when S is a composite object spanning all the figures shown in the dashboard and a component of K may have different compoents of S mapping into it slicing it in different ways.
\subsection{What are the lessons learned?}

\end{document}

