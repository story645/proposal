\documentclass[../main.tex]{subfiles}
\begin{document}
\section{Introduction}
\label{sec:intro}
\begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/math/dar.png}
    \caption{Visualization is equivariant maps between data and visual encoding of the variables and assembly of those encodings into a graphic.
    \note{will replace w/ overarching figure w/ same structure}}
    \label{fig:intro_artist_stages}
\end{figure}

While many researchers have identified and described important aspects of visualization, they have specialized in such different ways as to not provide a model general enough to natively support the full range of data and visualization types many general purpose modern visualization tools may need to support. The work presented in this paper is motivated by a need for a visualization library that developers could build complex, domain specific tools tuned to the semantics and structure carried in domain specific data. The core architecture also needs to be robust to the big data needs of many visualization practitioners, and therefore support distributed and streaming data needs. To support both exploratory and confirmatory visualization\cite{tukeyWeNeedBoth1980}, this tool needs to support 2D and 3D, static, dynamic and interactive visualizations. 

Specifically, this work was driven by a rearchitecture of the Python visualization library Matplotlib\cite{hunterMatplotlib2DGraphics2007} to meet modern data visualization needs. We aim to take advantage of developments in software design, data structures, and visualization to improve the consistency, composibility, and discoverability of the API. To do so, this work first presents a mathematical description of how data is transformed into graphic representations, as shown in figure~\ref{fig:intro_artist_stages}. As with other mathematical formalisms of visualization \cite{mackinlayAutomatingDesignGraphical1986,kindlmann2014algebraic,sugibuchiFramwork2009,vickersUnderstandingViz2013}, a mathematical framework provides a way to formalize the properties and structure of the visualization. In contrast to the other formalisms, the model presented here is focused on the components that build a visualization rather than the visualization itself. 

In other words this model is not intended to be evaluative, it is intended to be a reference specification for visualization library API. To make this model as implementation independent as possible, we propose fairly general mathematical abstractions of the data container such that we do not need to assume the data has any specific structure, such as a relational database. We reuse this structure for the graphic as that allows us to specifically discuss how structure is preserved. We take a functional approach because that gives implementers the flexibility to carry as little or as much state as they need and allows us to specify the constraints on these functions without having to know anything about the implementation or any state that these functions may be carrying. As with the other mathematical formalisms of visualization, we factor out the rendering into a separate stage; but our framework describes how these rendering instructions are generated. 

 In this work, we present a framework for understanding visualization as equivariant maps between topological spaces. Using this mathematical formalism, we can interpret and extend prior work and also develop new tools. We validate our model by using it to re-design artist and data access layer of Matplotlib, a general purpose visualization tool.
 

\section{Background}
Visual algorithms that display information are designed such that the structure of data is assumed, as described in Tory and MÃ¶ller's taxonomy \cite{toryRethinkingVisualizationHighLevel2004}. Specifically they note that discrete and continuous data and their attributes form a discipline independent design space \cite{pousmanCasualInformation2007}. This assumption is evident in the information visualization software design patterns categorized by Heer and Agrawala\cite{HeerSoftware2006} and can often be found in modern general purpose visualization libraries.  In proposing a new architecture, we contrast the trade offs libraries make, describe different types of data continuity, and discuss metrics by which a visualization library is traditionally evaluated. 

\subsection{Tools}
\label{sec:intro_data_tools}
The library driving this rearchitecture, Matplotlib, aims to be flexible enough that developers can build domain specific libraries on top of it. To preserve this flexibility, Matplotlib enforces minimal constraints on what sorts of data the user is allowed to input and supports very many visual algorithms, from primative marks to computationally complex visualizations. Instead of enforcing constraints at the API level, Matplotlib carries implicit assumptions about data continuity in how each function interfaces with the input data. This has lead to poor API consistency and brittle code as every visual algorithm has a very different point of view on how the data is structured. 

A commonly cited alternative is the family of tools built on top of Wilkenson's Grammar of Graphics (GoG) \cite{wilkinsonGrammarGraphics2005}, including ggplot\cite{wickhamGgplot2ElegantGraphics2016a}, protovis\cite{bostockProtoviz2009} and D3 \cite{bostockDataDrivenDocuments2011}, vega\cite{satyanarayanDeclarativeInteractionDesign2014} and altair\cite{vanderplasAltairInteractiveStatistical2018}. This framework is very popular in the data visualization community because it lends itself to a declarative interface \cite{heerDeclarative2010} which allows end users to describe the visualization they are trying to create.  Grammar oriented approaches describe how to compose visual elements into a graphical design \cite{wongsuphasawatNavigatingWideWorld2021}, while we are proposing a framework for building the elements.  

A different class of user facing tools are those that support images, such as ImageJ\cite{schneiderNIHImageImageJ2012}. These tools mostly have some support for visualizing non image components of a complex data set, but mostly in service to the image being visualized. These tools are ill suited for general purpose library developers as the architecture is to build plugins into the existing system \cite{WritingPlugins} rather than domain specific tools that are built on top. Even the digital humanities oriented macro ImagePlot\cite{studiesCulturevisImageplot2021}, which supports some non-image aggregate reporting charts, is still built around image data as the primary input. 

There are also visualization tools designed around scientifically complex data that support explicit description of the data, such as vtk\cite{hanwellVisualizationToolkitVTK2015,geveci2012vtk} and its derivatives such as MayaVi\cite{ramachandranMayaVi2011} and extensions such as ParaView\cite{ahrens2005paraview} and the infoviz themed Titan\cite{brianwylieUnifiedToolkitInformation2009}. These libraries, generally speaking, are architectured more as graphics libraries than visualization libraries, meaning they lack a clear distinction between the graphic construction and the rendering of that graphic. This is very close to the existing Matplotlib architecture, where the data, visual encoding, and rendering are jumbled in ways that make it hard to figure out what the code is doing. We are proposing a functional framework instead in large part to clearly encapsulate these separate responsibilities of a visualization tool. In turn, this should make it easier to reuse components in the building of new tools. 

\subsection{Data}
\label{sec:intro_data}
As mentioned, one of the drivers of this work was to facilitate building libraries that could natively support domain specific data containers. Fiber bundles are one such way model to model containers, as Butler proposes because they encode the continuity of the data separately from the types of variables and are flexible enough to support discrete and ND continuous datasets \cite{butlerVisualizationModelBased1989,butlerVectorBundleClassesForm1992}. Since Butler's model lacks a robust way of describing the variables, we fold in Spivak's Simplacial formulation of databases \cite{spivakDatabasesAreCategories2010,spivakSIMPLICIALDATABASES} so that we can encode a schema like description of the data in the fiber bundle.

\begin{figure}[H]
    \includegraphics[width=1\textwidth]{figures/intro/dataset_diagram.png}
    \caption{One way to describe data is by the connectivity of the points in the dataset. A database for example is often discrete unconnected points, while an image is an implicitely connected 2D grid. This image is from the Data Representation chapter of the MayaVi 4.7.2 documentation.\cite{DataRepresentationMayavi}}
    \label{fig:intro_data_format}
\end{figure}
As shown in figure~\ref{fig:intro_data_format}, there are many types of connectivity. A database typically consists of unconnected records, while an image is an implicit 2D grid and a network is some sort of explicitly connected graph.  In this work we will refer to the points of the dataset as \textit{records} to indicate that a point can be a vector of heterogenous elements. Each \textit{component} of the record is a single object, such as a temperature measurement, a color value, or an image. We also generalize \textit{component} to mean all objects in the dataset of a given type, such as all temperatures or colors or images. The way in which these records are connected is the \textit{connectivity}, \textit{continuity}, or more generally \textit{topology}.

\begin{mdframed}[roundcorner=10pt, frametitle= definitions, frametitlerule=true, frametitlebackgroundcolor=gray!10]
    \begin{description}
        \item[\textbf{records}] points, observations, entries 
        \item[\textbf{components}] variables, attributes, fields 
        \item[\textbf{connectivity}] how the records are connected to each other
    \end{description}
\end{mdframed}

Often this topology has metadata associated with it, describing for example when or where the measurement was taken. Building on the idea of metadata as \textit{keys} and their associated \textit{value} proposed by Munzner \cite{munznerChDataAbstraction}, we propose that information rich metadata are part of the components and instead the values are keyed on coordinate free structural ids. In contrast to Munzner's model where the semantic meaning of the key is tightly coupled to the position of the value in the dataset, our model allows for renaming all the metadata, for example changing the coordinate systems or time resolution, without imposing new semantics on the underlying structure.

\subsection{Visualization}
A visualization tool produces a graphical design and an image rendered based on that design, as described by Mackinlay \cite{mackinlayAUTOMATICDESIGNGRAPHICAL1987}. He defines the graphical design as the set of encoding relations from data to visual representation\cite{mackinlayAutomatingDesignGraphical1986}, and the design rendered in an idealized abstract space is what throughout this paper we will refer to as a graphic. In addition to the graphic representations, Byrne et al. describe how visualizations have figurative representations that have meaning due to their similarity in shape to external concepts \cite{byrneAcquiredCodesMeaning2016}. Mackinlay  proposes that a visualization tool's expressiveness is a measure of how much of the structure of the data the tool encodes, while the tool's effectiveness describes how much design choices are made in deference to perceptual saliency \cite{clevelandResearchStatisticalGraphics1987,clevelandGraphicalPerceptionTheory1984,chambersGraphicalMethodsData1983a, munznerVisualizationAnalysisDesign2014}.

\begin{figure}[H]
    \begin{subfigure}{.24\textwidth}
        \includegraphics[width=1\textwidth]{figures/intro/du_bois_spinny.png}
        \caption{}
        \label{fig:intro_dpa}
    \end{subfigure}
    \begin{subfigure}{.24\textwidth}
        \includegraphics[width=1\textwidth]{figures/intro/du_bois_bar.png}
        \caption{}
        \label{fig:intro_dpb}
    \end{subfigure}
    \begin{subfigure}{.24\textwidth}
        \includegraphics[width=1\textwidth]{figures/intro/du_bois_country.png}
        \caption{}
        \label{fig:intro_dbc}
    \end{subfigure}
    \begin{subfigure}{.24\textwidth}
        \includegraphics[width=1\textwidth]{figures/intro/du_bois_heat.png}
        \caption{}
        \label{fig:intro_dbd}
    \end{subfigure}
    \caption{Du Bois' data portraits\cite{duboiscenterattheuniversityofmassachusettsBoisDataPortraits2018} of post reconstruction Black American life exemplify that the fundemental characteristics of data visualization is that the visual elements vary in proportion to the source data. In figure~\ref{fig:intro_dpa}, the length of each segment maps to population; in figure~\ref{fig:intro_dpb}, the bar charts are intersected to show the number of property owners and how much they own in a given year; in figure~\ref{fig:intro_dbc} the countries are scaled to population size; and figure~\ref{fig:intro_dbd} is a treemap where the area of the rectangle is representative of the number of businesses in each field. The images here are from the Prints and Photographs collection of the Library of Congress \cite{duboisGeorgiaNegroCity1900,duboisGeorgiaNegroNegro1900, duboisSeriesStatisticalCharts, duboisSeriesStatisticalChartsa}}
    \label{fig:intro_dubois}
\end{figure}

We propose that the Du Bois visualizations in figure~\ref{fig:intro_dubois} are representative of the expressivity a core architecture should allow a downstream library to express. This is because while the Du Bois figures are not the common scatter, bar, or line plot \cite{friendlyBriefHistoryData2008}, they conform to the constraint that a visualization is a mapping from data to visuals. In figure~\ref{fig:intro_dbc}, Du Bois combines a graphical representation where glyph size varies by population with a figurative representation of those glyphs as the countries the data is from, which means that the semantic and numerical properties of the data are preserved in the graph. The visual representations in the Du Bois figures are in proportion to the quantitative data being represented, so the a chart is faithful according to Tufte's Lie Principal\cite{tufteVisualDisplayQuantitative2001}. The properties of the representation match the properties of the information being represented, so they are fairly understandable according to Norman's Naturalness Principal\cite{norman_things_smart}. As with the Du Bois data portraits, it is fundamental that any architecture tool we build ensures that the data properties match the visual properties. 


\begin{figure}[H]
\includegraphics[width=1\textwidth]{figures/intro/retinal_variables.png}
\caption{Retinal variables are a codification of how position, size, shape, color and texture are used to illustrate variations in the components of a visualization. The best to show column describes which types of information can be expressed in the corresponding visual encoding. This tabular form of Bertin's retinal variables is from Understanding Graphics \cite{malamedInformationDisplayTips2010} who reproduced it from \textit{Making Maps: A Visual Guide to Map Design for GIS} 
\cite{krygierMakingMapsVisual2005}}
\label{fig:intro_retinal_variables}
\end{figure}

The visual properties were first codified by Bertin\cite{bertinSemiologyGraphicsDiagrams2011a}, who also described the types of data that paired with the visual properties shown in figure~\ref{fig:intro_retinal_variables}. It is at this encoding level that Mackinlay formalized expressiveness as a homomorphic mapping which preserves some binary operator from one domain to another \cite{mackinlayAUTOMATICDESIGNGRAPHICAL1987}. A functional dependency framework for evaluating these equivariances was proposed by Sugibuchi et al \cite{sugibuchiFramwork2009}, and an algbraic basis for visualization design and evaluation was proposed by Kindlmann and Scheideggar\cite{kindlmann2014algebraic}. Vickers et al. propose a category theory framework\cite{vickersUnderstandingViz2013} that extends the equivariance checks to layout, but is focused strictly on the design layer like the other mathematical frameworks. 

The visual encodings are composited into the point, line, and area graphical marks, as shown in figure~\ref{fig:intro_retinal_variables}. Marks can be generalized to glyphs, which are graphical objects that convey one or more attributes of the data entity mapped to it\cite{ware2019information, munznerVisualizationAnalysisDesign2014} and minimally need to be differentiable from other visual elements \cite{ziemkiewiczEmbeddingInformationVisualization2009}. As retinal variables and marks are the building blocks of most visualizations, they must be fully expressible in a framework for building visualization tools. By building visualization concepts into the core architecture, developers can incorporate assessments of the visualization,  such as as quality metrics\cite{bertiniQualityMetricsHighdimensional2011a} or invariance \cite{kindlmann2014algebraic} of visualizations with respect to graphical encoding choices.  

\subsection{Contribution}
This work presents a mathematical model of the transformation from data to graphic representation and a proof of concept implementation. Specifically, this work contributes

\begin{enumerate}
    \item a functional oriented visualization tool architecture
    \item topology-preserving maps from data to graphic 
    \item monoidal action equivariant maps from component to visual variable 
    \item algebraic sum such that more complex visualizations can be built from simple ones 
    \item prototype built on Matplotlib's infrastructure
\end{enumerate}
In contrast to mathematical models of visualization that aim to evaluate visualization design, we propose a topological framework for building tools to build visualizations. We defer judgement of expressivity and effectiveness to developers building domain specific tools, but provide them the framework to do so. 
\end{document}